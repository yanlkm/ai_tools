# Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) 

This is a simple implementation from scratch of an RNN and LSTM to analyze the issue of vanishing/exploding gradients.
The dataset used is the **history of the weather means** in several countries across the years. 

## Quick summary of files architecture
- `rnn.py`: Implementation of a simple RNN.
- `lstm.py`: Implementation of a simple LSTM.
- `utils.py`: Utility functions for the data formatting.
- `main.py`: Main file to train the RNN and LSTM and show the gradients norms evolution.

## Presentation of a regular RNN

A Recurrent Neural Network (RNN) is a type of neural network that is designed to handle sequential data. Unlike traditional feedforward neural networks (see my [NEURAL NETWORK IN C](https://github.com/yanlkm/ai_tools/tree/main/nnc) for more details), RNNs introduce a mechanism that allows them to retain information from previous time steps, making them perfect for tasks where temporal dependencies are important, such as time series, natural language processing, and speech recognition.

### 1. Input Layer
Each time step of the sequence is processed **individually**, and the output at each step depends not only on the current input but also on the **hidden state** from the previous steps.

### 2. Hidden State

The hidden state acts as the memory of the network, allowing it to retain information about previous time steps. At each time step $t$, the hidden state is updated using the current input and the hidden state from the previous step :
$$
h_t = f(W_{ih} x_t + W_{hh} h_{t-1})
$$
where:
- $h_t$ is the hidden state at time step $t$.

In the implemented RNN, the hidden state is updated using:

$$
h_t = \tanh(W_{ih} x_t + W_{hh} h_{t-1})
$$
where $W_{ih}$ and $W_{hh}$ are the weight matrices for the input-to-hidden and hidden-to-hidden connections, respectively.

### 3. Output Layer
After computing the hidden state, the RNN generates an output for the current time step using: 

$$
y_t = W_{ho} h_t
$$
where $W_{ho}$ is the weight matrix for the hidden-to-output connection.

### 4. Activation Functions
The RNN uses a tanh activation function to compute the new hidden state. This helps to introduce non-linearity and ensures that the hidden state values remain bounded.

### Details of the PyTorch Implementation
#### 1. Key Components

* *Input-to-Hidden Layer* (`input_to_hidden`): A fully connected layer **without bias** that transforms the input vector into the hidden state space.
* *Hidden-to-Hidden Layer* (`hidden_to_hidden`): A fully connected layer that updates the hidden state based on the previous hidden state (sum of the input-to-hidden and hidden-to-hidden connections).
* *Hidden-to-Output Layer* (`hidden_to_output`): A fully connected layer that generates the output from the hidden state.

#### 2. Initialization of Hidden State

The hidden state is initialized to a tensor of zeros for the first time step, using the `hidden_to_zeros` method. This ensures that the network starts with no prior memory.

#### 3. Forward Pass

The forward pass consists of the following steps:

1. The input is transformed into the hidden state space using `input_to_hidden`.
2. The **previous** *hidden state* is updated using `hidden_to_hidden`.
3. The **new hidden state** is computed by applying the *tanh* activation function to the sum of the transformed input and the updated hidden state.
4. The output is generated by applying `hidden_to_output` to the *new hidden state*.


## Presentation of a Long Short-Term Memory (LSTM)

A Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) that is designed to handle long-term dependencies and **mitigate** the **vanishing/exploding gradient problem**. LSTMs introduce a *more complex architecture* with **gates** that control the flow of information.

### 1. Input Layer
Each time step of the sequence is processed individually, and the output at each step depends on the **current input**, **the hidden state**, and the **cell state** from the *previous steps*.

### 2. Hidden State and Cell State

The **hidden state** acts as the *short-term memory* of the network and the **cell state** acts as *the long-term memory*. At each time step $t$ , the *hidden state* and *cell state* are updated using the current input and the previous states.

### 3. Gates
LSTMs use **three** gates to control the flow of information:

- **Forget Gate**: Decides what information to **remove** from the cell state (long-term memory).

$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
$$

- **Input Gate**: Decides what new information to **store* in the cell state (long-term memory).

$$
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
$$

$$
\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)
$$

- **Output Gate**: Decides what information to **output** from the hidden state (short-term memory).

$$
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
$$

### 4. Cell State Update
The *cell state* is updated using the forget gate and input gate:

$$
C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
$$

### 5. Hidden State Update
The *hidden state* is updated using the *output gate* and the *updated cell state*:

$$
h_t = o_t \odot \tanh(C_t)
$$

### Details of the PyTorch Implementation
#### 1. Key Components

* *Forget Gate* (`W_f`, `b_f`): Determines what information to *remove* from the cell state.
* *Input Gate* (`W_i`, `b_i`): Determines what new information to *store* in the cell state.
* *Cell State Candidate* (`W_C`, `b_C`): *Generates* new candidate values for the cell state.
* *Output Gate* (`W_o`, `b_o`): Determines what information to *output* from the hidden state.
* *Hidden-to-Output Layer* (`hidden_to_output`): *Generates the output* from the hidden state.

#### 2. Initialization of Hidden and Cell States

The hidden state and cell state are initialized to tensors of zeros for the first time step using the `init_hidden_and_cell` method. This ensures that the network starts with no prior memory.

#### 3. Forward Pass

The forward pass consists of the following steps:

1. The *input* and *previous hidden* state are **combined**.
2. The *forget gate* determines what information to **remove** from the cell state.
3. The *input gate* determines what new information to **store** in the cell state.
4. The *cell state* is updated using the **forget gate and input gate**.
5. The *output gate* determines what information to **output** from the *hidden state*.
6. The *hidden state* is updated using the **output gate** and the **updated cell state**.
7. The *output* is generated by applying `hidden_to_output` to the **new hidden state**.


## Conclusion

The implementation of the RNN and LSTM provides an understanding of how these networks work and how they address the vanishing/exploding gradient problem. By analyzing the gradients during training, we can observe how the LSTM architecture helps to maintain stable gradients and improve the learning process. 

